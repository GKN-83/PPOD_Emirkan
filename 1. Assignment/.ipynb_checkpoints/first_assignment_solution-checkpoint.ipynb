{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #1: Pseudonymisation Techniques and Considerations\n",
    "- Dataset: Crossfit [Daset](https://data.world/bgadoci/crossfit-data) (In this assignment only the athletes file was used) \n",
    "- Credits: Dataset was put together by Sam Swift\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb3045c94d79fd90"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julius\\AppData\\Local\\Temp\\ipykernel_13812\\1944461800.py:4: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataframe = pd.read_csv(\"athletes.csv\")\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "import pandas as pd\n",
    "\n",
    "# Read csv as dataframe\n",
    "dataframe = pd.read_csv(\"athletes.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T11:05:07.906521700Z",
     "start_time": "2023-11-12T11:05:06.772155300Z"
    }
   },
   "id": "be93d85a454a3b2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Pseudonymisation\n",
    "Goals:\n",
    "-  Determine which attributes qualify as explicit personally identifiable information (3.1.1)\n",
    "    - Why? \n",
    "    - What method was used to identify these? \n",
    "- Generate pseudonymous values for the identified attributes (3.1.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa68ec633060dcf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Identifying Attributes containing Personally Identifiable Information (PII)\n",
    "- In order to identify th attributes first a better understanding of the structure of the dataset needs to be obtained\n",
    "    - What columns does the dataset contain and in what format are the attribute values?\n",
    "        - Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a09a389f01e7c40"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 'athlete_id', Example Data: 2554.0\n",
      "Column: 'name', Example Data: Pj Ablang\n",
      "Column: 'region', Example Data: South West\n",
      "Column: 'team', Example Data: Double Edge\n",
      "Column: 'affiliate', Example Data: Double Edge CrossFit\n",
      "Column: 'gender', Example Data: Male\n",
      "Column: 'age', Example Data: 24.0\n",
      "Column: 'height', Example Data: 70.0\n",
      "Column: 'weight', Example Data: 166.0\n",
      "Column: 'fran', Example Data: 211.0\n",
      "Column: 'helen', Example Data: 645.0\n",
      "Column: 'grace', Example Data: 300.0\n",
      "Column: 'filthy50', Example Data: 1053.0\n",
      "Column: 'fgonebad', Example Data: 0.0\n",
      "Column: 'run400', Example Data: 61.0\n",
      "Column: 'run5k', Example Data: 1081.0\n",
      "Column: 'candj', Example Data: 220.0\n",
      "Column: 'snatch', Example Data: 200.0\n",
      "Column: 'deadlift', Example Data: 400.0\n",
      "Column: 'backsq', Example Data: 305.0\n",
      "Column: 'pullups', Example Data: 25.0\n",
      "Column: 'eat', Example Data: I eat 1-3 full cheat meals per week|\n",
      "Column: 'train', Example Data: I workout mostly at a CrossFit Affiliate|I have a coach who determines my programming|I record my workouts|\n",
      "Column: 'background', Example Data: I played youth or high school level sports|I regularly play recreational sports|\n",
      "Column: 'experience', Example Data: I began CrossFit with a coach (e.g. at an affiliate)|I have attended one or more specialty courses|I have had a life changing experience due to CrossFit|\n",
      "Column: 'schedule', Example Data: I do multiple workouts in a day 2x a week|\n",
      "Column: 'howlong', Example Data: 4+ years|\n",
      "Column: 'retrieved_datetime', Example Data: 2015-03-01 22:49:18\n"
     ]
    }
   ],
   "source": [
    "def get_first_not_not_empty_value(df_column):\n",
    "    return df_column.dropna().iloc[0] if not df_column.dropna().empty else None\n",
    "\n",
    "# Iterate each column \n",
    "for column in dataframe.columns:\n",
    "    first_value = get_first_not_not_empty_value(dataframe[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T11:05:08.251336300Z",
     "start_time": "2023-11-12T11:05:07.909528600Z"
    }
   },
   "id": "9e56d29d57a964b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- By inspecting the different columns and the data format, several attributes which have the potential to contain explicit personally identifiable information can be identified\n",
    "    - `athelete_id`\n",
    "        -  This really depends on the usage of this id! Considerations to take into account are: \n",
    "            - Is the `athlete_id` only used as an internal id of this dataset or does it maybe even refer to an official id?\n",
    "            - Are there other datasets available which may have a similar source to this dataset? Thus, these other datasets may use the same `athlete_id`\n",
    "    - `name`\n",
    "        - The name allows to identify an individual\n",
    "    - `team`\n",
    "        - Depending on the size of the team, this could allow to identify a specific athlete\n",
    "    - `affiliate` \n",
    "        - Depending on the affiliate and the amount of contracted athletes, this could allow to identify an individual\n",
    "    - All stats of the athletes\n",
    "        - If an athlete has really remarkable stats (maybe even a world record in a category), this could allow to identify the individual\n",
    "    - `train` \n",
    "        - If an athlete has a special and famous training routine, this could allow to identify him\n",
    "    - `background`\n",
    "        - If an athlete has a famous background or mentions names, this could allow to identify him\n",
    "    - `experience`\n",
    "        - If an athlete mentions concrete information about his experience (e.g. name of current coach), this could allow to identify him\n",
    "\n",
    "-> As can be seen, all columns could potentially contain outliers which could be then used to identify an individual. As agreed on, for each of the columns (only those holding numbers or names) potentially leaking PII one anonymization technique was chosen which keeps the data loss as minimal as possible without leaking obvious PII. It is important to note, that there could still be columns containing PII. However, this chance for outliers was accepted in this assignment. Columns containing descriptions (e.g. train, background and experience) where to anonymized and thus the chance for outliers accepted."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ed2670d051f539a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Pseudonymizing\n",
    "- For pseudonymisation especially the name is suitable, because it can be easily replaced with a fake name, without being as obvious or destroying the purpose of the dataset\n",
    "- With the help of the `anonymizedf` library new fake names can be created for the `name` column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba2e7f8a58b909df"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install anonymizedf\n",
    "\n",
    "from anonymizedf import anonymizedf\n",
    "\n",
    "# Prepare the data to be anonymized\n",
    "an = anonymizedf.anonymize(dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T11:07:28.047452800Z",
     "start_time": "2023-11-12T11:07:28.035320200Z"
    }
   },
   "id": "8e1f75b29dbfdecd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add new column with fake name\n",
    "an.fake_names(\"name\")\n",
    "\n",
    "# Drop old original name column\n",
    "dataframe = dataframe.drop(columns=['name'])\n",
    "\n",
    "# Rename Fake_name column in name\n",
    "dataframe = dataframe.rename(columns={'Fake_name': 'name'})\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.270284700Z"
    }
   },
   "id": "29c62fdff3df03bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Randomisation\n",
    "Goal:\n",
    "- Use randomisation technique to generate random strings (no meaning) (3.2.1)\n",
    "- Use randomisation technique to generate random but meaningful replacements (3.2.2)\n",
    "- Create a lookup table to keep track of the changes (3.2.3)\n",
    "\n",
    "### 3.2.1 Random Strings\n",
    "1. Identify columns to replace with random strings\n",
    "    - Here columns containing words can be used \n",
    "        - `name`\n",
    "        - `region`\n",
    "        - `team`\n",
    "        - `affiliate` \n",
    "2. Generate a function which is responsible for generating random values for replacement\n",
    "3. Overwrite each value in respective attributes by using the generated value"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d638b8091b0fee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Define function to create random string\n",
    "def generate_random_string(length):\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "# Apply randomization on specified columns\n",
    "dataframe['name'] = dataframe['name'].apply(lambda x: generate_random_string(10))\n",
    "dataframe['region'] = dataframe['region'].apply(lambda x: generate_random_string(15))\n",
    "dataframe['team'] = dataframe['team'].apply(lambda x: generate_random_string(20))\n",
    "dataframe['affiliate'] = dataframe['affiliate'].apply(lambda x: generate_random_string(25))\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.271472900Z"
    }
   },
   "id": "aea49738582c6a6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.2 Randomization with meaningful strings\n",
    "1. Identify columns suitable\n",
    "    - `name`\n",
    "        -  For randomizing the name, the first letter will be kept equal to the original in order to keep similarities\n",
    "    - `region`, `team`, `affiliate`\n",
    "        - The values in the dataset for each of the columns will be interchanged. Thus, the data is still correct and for example a team with the respective name still exists \n",
    "2. Write functionality to randomize the values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e72fe81893e96ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from anonymizedf import anonymizedf\n",
    "\n",
    "# To see the changes properly we have to reload the initial data\n",
    "dataframe = pd.read_csv(\"athletes.csv\")\n",
    "        \n",
    "# Randomization the name\n",
    "# insert your functionality Emirkan\n",
    "\n",
    "\n",
    "# Interchange values for region, team and affiliate\n",
    "def interchange_values(column_name):\n",
    "    dataframe[column_name] = dataframe[column_name].sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "interchange_values('region')\n",
    "interchange_values('team')\n",
    "interchange_values('affiliate')\n",
    "\n",
    "print(dataframe.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.273053700Z"
    }
   },
   "id": "2a5c4e934ac55241"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 Save mappings\n",
    "- Rewrite the previous functionalities (Only for the 3.2.2) to save the mapping between old value and new value in an additional table"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a17416c70366c59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To see the changes properly we have to reload the initial data\n",
    "dataframe = pd.read_csv(\"athletes.csv\")\n",
    "\n",
    "# Copy the original values in mapping dataframe\n",
    "dataframe_mapping = dataframe[['name', 'region', 'team', 'affiliate']].copy()\n",
    "\n",
    "# Randomize name\n",
    "# @Emirkan: Insert here to functionality to randomize name \n",
    "\n",
    "# Randomize region, team and affiliate\n",
    "interchange_values('region')\n",
    "interchange_values('team')\n",
    "interchange_values('affiliate')\n",
    "\n",
    "# Copy the modified values in mapping dataframe\n",
    "dataframe_mapping[['randomized_region', 'randomized_team', 'randomized_affiliate']] = dataframe[['region', 'team', 'affiliate']].copy()\n",
    "\n",
    "print(dataframe_mapping.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.274259500Z"
    }
   },
   "id": "41a1b42d28b3b8d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Aggregation\n",
    "Goal:\n",
    "- Determine attributes which qualify for aggregation (3.3.1)\n",
    "- Write a function to perform that aggregation process (3.3.2)\n",
    "\n",
    "### 3.3.1 Determine Attributes for Aggregation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c59dc5a07acdb83d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the each column with its first not empty value\n",
    "for column in dataframe.columns:\n",
    "    first_value = get_first_not_not_empty_value(dataframe[column])\n",
    "    print(f\"Column: '{column}', Example Data: {first_value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.275266300Z"
    }
   },
   "id": "cdfc0b43c8b43a28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Numerical attributes can be easily aggregated\n",
    "- Especially those related to information about the individual are suitable for aggregation\n",
    "    - `age`\n",
    "    - `height` \n",
    "    - `weight`\n",
    "- To be able to identify different levels of values, the extremes have to be identified "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c32c31ee64889069"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Minimum age': {dataframe['age'].nsmallest(5)}, Maximum age: {dataframe['age'].nlargest(5)}\")\n",
    "print(f\"Minimum height': {dataframe['height'].nsmallest(10)}, Maximum height: {dataframe['height'].nlargest(10)}\")\n",
    "print(f\"Minimum weight': {dataframe['weight'].nsmallest(10)}, Maximum weight: {dataframe['weight'].nlargest(10)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.277272100Z"
    }
   },
   "id": "dabc635f8ab91128"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Because even by observing ten extremes for the height and weight the values didn't make sense, in the following average values for the entire population were taken"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c523edd6a9c147"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bins_age = [0, 18, 30, 45, 60, 100]\n",
    "labels_age = ['0-18', '19-30', '31-45', '46-60', '60+']\n",
    "\n",
    "bins_height = [0, 159, 169, 179, 189, 199, 220]\n",
    "labels_height = ['0-159', '160-169', '170-179', '180-189', '190-199', '200+']\n",
    "\n",
    "bins_weight = [0, 49, 59, 69, 79, 89, 99, 130]\n",
    "labels_weight = ['0-49', '50-59', '60-69', '70-79', '80-89', '90-99', '100+']\n",
    "\n",
    "dataframe['age'] = pd.cut(dataframe['age'], bins=bins_age, labels=labels_age)\n",
    "dataframe['height'] = pd.cut(dataframe['height'], bins=bins_age, labels=labels_age)\n",
    "dataframe['weight'] = pd.cut(dataframe['weight'], bins=bins_age, labels=labels_age)\n",
    "\n",
    "print(dataframe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.278686800Z"
    }
   },
   "id": "3a37f1ccdf7063b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Perturbation\n",
    "Goal:\n",
    "- Select attributes to add noise to (3.4.1)\n",
    "- Implement the functionality to add the noise (3.4.2)\n",
    "    - The original distribution of values should be preserved \n",
    "- Analyse distribution of original data and then with noise added (3.4.3)\n",
    "\n",
    "### 3.4.1 Select attributes\n",
    "- Similar to the aggregation, perturbation as well fits best for numbers as attribute values \n",
    "- Because the stats of the athletes are the main subject of the dataset they should normally be no noise added to them\n",
    "    - Even more, the correctness of the value really matters in order to be able to compare different athletes. Here already a small noise is not good\n",
    "- However, as already mentioned in 3.1 this kind of information loss is accepted in this assignment\n",
    "- Therefore, the stats will be pertubated \n",
    "\n",
    "### 3.4.2 Pertubate Values\n",
    "- Standard deviation was used to determine noise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff1f6ae44fdd428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_noise(df, column, std = None):\n",
    "    if std == None:\n",
    "        std = df[column].std()\n",
    "    \n",
    "    withNoise = df[column].add(np.random.normal(0, std, df.shape[0]))\n",
    "    copy = df.copy()\n",
    "    copy[column] = withNoise\n",
    "    return copy\n",
    "\n",
    "dataframe_pertubation = add_noise(dataframe, \"helen\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"grace\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"filthy50\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"fgonebad\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"run400\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"run5k\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"candj\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"snatch\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"deadlift\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"backsq\")\n",
    "dataframe_pertubation = add_noise(dataframe_pertubation, \"pullups\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.279195200Z"
    }
   },
   "id": "f258df58c6fdb35f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 Show Distributions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e6abda3ade098b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(20,10))\n",
    "ax1.hist(\n",
    "    'helen', \n",
    "    data=dataframe, \n",
    "    bins = np.arange(start=100, stop=2_000, step=200),\n",
    ")\n",
    "ax1.title.set_text('Original Distribution')\n",
    "ax2.hist(\n",
    "    'helen', \n",
    "    data=dataframe_pertubation, \n",
    "    bins = np.arange(start=100, stop=2_000, step=200),\n",
    ")\n",
    "ax2.title.set_text('Distribution with noise added')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-12T11:05:08.280200700Z"
    }
   },
   "id": "b09bc399eb38950e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Data Analysis\n",
    "Goal:\n",
    "- Determine data loss using function (3.5.1)\n",
    "- Discuss pros and cons (3.5.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3084722e47e63b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
