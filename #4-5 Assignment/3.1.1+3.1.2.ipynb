{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment #4-5: Anonymising Textual Data and De-Anonymisation\n",
    "- Dataset:  Tweets Emotions [Dataset](https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text?resource=download)\n",
    "- Credits: Dataset was put together by Pashipatu Gupta\n",
    "- ToDo: To run the jupyter notebook the requirements.txt need be installed (`pip install -r requirements.txt`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee42f8ae463ded8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Textual Data Anonymisation – 30 marks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7c1a5b1a1bad415"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Do some research to determine what needs to be anonymised in the data and why.\n",
    "- For a better understanding of the structure of the dataset , we display the attribute values\n",
    "- What columns does the dataset contain and in what format are the attribute values?\n",
    "Therefore, each column and the first value of each column (which is not empty or Null) is printed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "237a8e23efc1b84d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")\n",
    "print(df.iloc[:4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:19:12.490080Z",
     "start_time": "2024-03-08T09:19:12.450979Z"
    }
   },
   "id": "74a9cb538af3b384"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By inspecting the different columns and the data format, the 'content' attribute definitely has the potential to contain explicit personally identifiable information:\n",
    "1. User Mentions: \n",
    "    - Any instance of @username should be anonymised because it directly points to an individual's account, which is considered personally identifiable information (PII).\n",
    "2. First Names: \n",
    "    - If any first names are used in a context that can identify an individual, such as tagging in combination with other identifying information, they should be anonymised.\n",
    "3. Locations and Specific References: \n",
    "    - Any mention of specific locations, addresses, landmarks, or establishments that could help in identifying an individual should be anonymised.\n",
    "4. Specific Events with Identifiable Information: \n",
    "    - References to specific events that may lead to the identification of individuals, like parties or gatherings with a list of names, should be anonymised.\n",
    "5. Unique Identifiers: \n",
    "    - Any other unique identifiers, such as specific dates, times, or unique events, that could potentially be linked back to an individual.\n",
    "\n",
    "\n",
    "Apart from that, the 'sentiment' attribute is explored further as we don't know by now how many unique values there actually are and if they would qualify as PII: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190ed3635d49ace1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of the dataframe:  40000\n",
      "number of unique values in sentiment:  13\n",
      "counts per unique value in sentiment\n",
      "neutral       8638\n",
      "worry         8459\n",
      "happiness     5209\n",
      "sadness       5165\n",
      "love          3842\n",
      "surprise      2187\n",
      "fun           1776\n",
      "relief        1526\n",
      "hate          1323\n",
      "empty          827\n",
      "enthusiasm     759\n",
      "boredom        179\n",
      "anger          110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"total lenth of the dataframe: \", len(df))\n",
    "\n",
    "# Calculate the number of unique values and the number of entries per unique value\n",
    "unique_counts = df['sentiment'].nunique()\n",
    "value_counts = df['sentiment'].value_counts()\n",
    "\n",
    "print(\"number of unique values in sentiment: \", unique_counts)\n",
    "print(\"counts per unique value in\", value_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:19:12.496493Z",
     "start_time": "2024-03-08T09:19:12.490751Z"
    }
   },
   "id": "7409df0e76827242"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By inspecting the 'sentiment' attribute further, we can say that there are 13 different values in the 'sentiment' column. We also know that there are 40,000 tweets in total in the dataset. Given this information, there is no need to anonymize the 'sentiment' attribute."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48910199218c1788"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Using a Natural Language Processing library (e.g. Python’s spaCy), analyse the text to identify elements of personally identifiable information (PII).\n",
    "The goal of anonymization is to remove or obscure such details so that the individuals to whom the data pertains cannot be readily identified. The first step is finding the contents, that might actually contain PII.\n",
    "As the first step, we install 'en_core_web_lg', a pre-trained spaCy model suitable for identifying named entities, which include PII. 'en_core_web_lg' is the English model trained on web text. It has been trained on a diverse range of web text, including blogs, news, comments. We've decided on using 'en_core_web_lg' instead of for example 'en_core_web_trf' due to their balance between performance and resource usage.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ed5640c245c74b6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg > /dev/null 2>&1 #install model without outputting in console"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:19:12.499201Z",
     "start_time": "2024-03-08T09:19:12.495931Z"
    }
   },
   "id": "2c37a5979e8eebe7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#Load the large, pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Function to identify PII using spaCy\n",
    "def identify_pii(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    pii_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pii_entities\n",
    "\n",
    "pii_original = df['content'].apply(identify_pii)\n",
    "\n",
    "df['PII'] = pii_original\n",
    "df.to_csv(\"PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:22:26.313804Z",
     "start_time": "2024-03-08T09:19:12.499476Z"
    }
   },
   "id": "7aeb8f4d70fdbdfd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
      "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
      "\n",
      "                           PII  \n",
      "0  [('@tiffanylue', 'PERSON')]  \n",
      "1                           []  \n",
      "2         [('friday', 'DATE')]  \n",
      "3                           []  \n",
      "4         [('Houston', 'GPE')]  \n"
     ]
    }
   ],
   "source": [
    "#load new dataframe containing the PII information\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")\n",
    "print(df.iloc[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:22:26.366874Z",
     "start_time": "2024-03-08T09:22:26.319490Z"
    }
   },
   "id": "b01646ec40fd6a5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each non-empty list within the square brackets [] in the new 'PII' column indicates that the spaCy model has identified text segments in that specific row which it believes to be named entities. The entities are tagged with labels that classify what type of entity they are (e.g., DATE, PERSON, ORG(=organization), GPE(=Geopolitical Entity)). These named entities can be considered PII, as they might be used to identify an individual either directly or when combined with other additional information. In conclusion, when there is a non-empty list in the 'PII' column in a specific row, we might have to apply some sort of anonymisation mechanism to prevent the PIIs from being able to identify an individual."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123e44b1e28173f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a further step towards the next task, we check how many occurrences of which category we have in our new PII column. This information is crucial for planning further anonymization steps. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b1fa0fa40642cb9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:22:26.420054Z",
     "start_time": "2024-03-08T09:22:26.364047Z"
    }
   },
   "id": "bb79a26e9d3b877f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PERSON': 10340, 'ORG': 8815, 'DATE': 7560, 'CARDINAL': 3430, 'GPE': 3269, 'TIME': 2964, 'NORP': 957, 'PRODUCT': 702, 'ORDINAL': 680, 'WORK_OF_ART': 607, 'MONEY': 353, 'LOC': 244, 'FAC': 210, 'QUANTITY': 191, 'EVENT': 123, 'LANGUAGE': 106, 'PERCENT': 75, 'LAW': 35})\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import ast \n",
    "\n",
    "# Function to extract entities from a string and return their labels\n",
    "def extract_labels(data_string):\n",
    "    # Convert string representation of list to actual list\n",
    "    entities = ast.literal_eval(data_string)\n",
    "    # Extract labels\n",
    "    return [label for _, label in entities]\n",
    "\n",
    "# Extract labels from each item in the data\n",
    "all_labels = [label for item in df['PII'] for label in extract_labels(item)]\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = collections.Counter(all_labels)\n",
    "\n",
    "print(label_counts)\n",
    "print(len(label_counts))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:22:26.681959Z",
     "start_time": "2024-03-08T09:22:26.478563Z"
    }
   },
   "id": "5c5a7818bbd0a1e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 18 different categories have the following meaning: \n",
    "\n",
    "- PERSON: Names of people.\n",
    "- ORG: Organizations, including companies, agencies, institutions, etc.\n",
    "- DATE: Absolute or relative dates or periods.\n",
    "- CARDINAL: Numerals that do not fall under another type (like dates or quantities).\n",
    "- GPE: Geopolitical entity, typically referring to countries, cities, states.\n",
    "- TIME: Times smaller than a day, including specific time periods, durations, or times of day.\n",
    "- NORP: Nationalities, religious or political groups.\n",
    "- ORDINAL: \"First\", \"second\", etc., used to denote position in a ordered sequence.\n",
    "- PRODUCT: Objects, vehicles, foods, etc. (not services).\n",
    "- MONEY: Monetary values, including unit.\n",
    "- WORK_OF_ART: Titles of books, songs, etc.\n",
    "- LOC: Non-GPE locations, mountain ranges, bodies of water.\n",
    "- FAC: Facilities, including buildings, airports, highways, bridges, etc.\n",
    "- QUANTITY: Measurements, as of weight or distance.\n",
    "- EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
    "- PERCENT: Percentage (including \"%\").\n",
    "- LANGUAGE: Any named language.\n",
    "- LAW: Named documents made into laws.\n",
    "\n",
    "So, we now know that there are 18 types of different datatypes that might have to be anonymized in some kind of way.\n",
    "We will now take a closer look at what is actually behind those categories in practical application:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e4c20a822e04f0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: ['@tiffanylue', 'BC', '@charviray Charlene', 'isaac', '@davidbrussee']\n",
      "ORG: ['@BrodyJenner if u watch the hills in london u', 'itonlinelol', '@annarosekerr', '@PerezHilton', 'Program']\n",
      "DATE: ['friday', 'Friday', 'weeks', 'weeks', '2009']\n",
      "CARDINAL: ['2', '#', '2', '80', '6hrs']\n",
      "GPE: ['Houston', 'SoCal', 'canada', 'Cali', '@lostluna']\n",
      "TIME: ['this afternoon', 'this afternoon', '9am', 'nightly', '6am']\n",
      "NORP: ['@Telstra', 'French-', 'Malay', 'Aussie', 'Christian']\n",
      "ORDINAL: ['first', 'first', 'first', 'First', 'second']\n",
      "PRODUCT: ['Blackberry', '@Pokinatcha', 'Windows', 'veronica', '@judyrey']\n",
      "MONEY: ['#uds', '20', '100', '3wordsaftersex', '#beer #']\n",
      "WORK_OF_ART: ['Horse Pills', 'Drag Me to Hell', 'The Biggest Loser on Hallmark', 'The Biggest Loser', 'Dead Like Me']\n",
      "LOC: ['Voobys', 'Torchwood', 'Harpers Island', 'the east coast', 'HII']\n",
      "FAC: ['the Balisage Markup Conference', 'the Hongkong International Airport', 'WII', '@xMyLifesAStoryx', 'Cork Airport']\n",
      "QUANTITY: ['400MB', '1000+ miles', '5 secs  yesterdaynight knijp', '7 pounds', '2 mc!']\n",
      "EVENT: ['Open Day', 'Stanley Cup Finals', '@laurelexmachina Swimming', 'the Chasers War', 'Summer Glau']\n",
      "PERCENT: ['1.55%', '0%', '100%', '6%', '90%']\n",
      "LANGUAGE: ['english', 'english', '@yahyan', '@yahyan', 'english']\n",
      "LAW: ['Chapter 1', '@leopardqueen @seekinspiration', 'chapter 27', 'chapter 73', 'CGI 09']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "categories = [\n",
    "    'PERSON', 'ORG', 'DATE', 'CARDINAL', 'GPE', 'TIME', 'NORP', 'ORDINAL',\n",
    "    'PRODUCT', 'MONEY', 'WORK_OF_ART', 'LOC', 'FAC', 'QUANTITY', 'EVENT', \n",
    "    'PERCENT', 'LANGUAGE', 'LAW'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to hold the first five examples of each category\n",
    "first_five_examples = {category: [] for category in categories}\n",
    "\n",
    "# Iterate over the DataFrame to find examples\n",
    "for index, row in df.iterrows():\n",
    "    # Convert the string representation of list to actual list\n",
    "    entities = ast.literal_eval(row['PII'])\n",
    "\n",
    "    # Check for the first five examples of each category\n",
    "    for entity, label in entities:\n",
    "        if label in categories and len(first_five_examples[label]) < 5:\n",
    "            first_five_examples[label].append(entity)\n",
    "\n",
    "    # Break the loop if five examples have been found for all categories\n",
    "    if all(len(examples) == 5 for examples in first_five_examples.values()):\n",
    "        break\n",
    "\n",
    "# Print the examples\n",
    "for category, examples in first_five_examples.items():\n",
    "    print(f\"{category}: {examples}\")\n",
    "\n",
    "# If some categories are missing or have less than five examples, print a message\n",
    "for category, examples in first_five_examples.items():\n",
    "    if len(examples) < 5:\n",
    "        print(f\"Less than five examples found for category: {category}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T09:22:26.798348Z",
     "start_time": "2024-03-08T09:22:26.683836Z"
    }
   },
   "id": "173c22de68946beb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After looking at the output of the different categories, we've decided on anonymizing the following eight categories in the next task, because they might lead to the identification of an individual: \n",
    "\n",
    "- PERSON, GPE, DATE, ORG, NORP, CARDINAL, ORDINAL, TIME\n",
    "\n",
    "After further inspection of the dataset, the remaining 10 categories don't seem to be problematic in context of PII. \n",
    "\n",
    "Please continue reading in 3.1.3+3.1.4.ipynb  :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "670f518f76722a4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
