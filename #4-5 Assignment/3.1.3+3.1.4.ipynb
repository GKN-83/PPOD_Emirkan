{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Using the techniques you applied in Assignment #1, apply a masking or transformation mechanism to modify the detected PII elements and substitute with suitable replacements.\n",
    "In the following section, we will apply techniques similar to those used in Assignment #1 to mask or transform Personally Identifiable Information (PII) detected in a dataset. The goal is to substitute these sensitive elements with suitable replacements while maintaining the overall structure and coherence of the data. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf64464ca6c1a3f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by anonymizing the eight categories with help from the Faker library:\n",
    "- PERSON: Replaces names of people with fake names generated by Faker.\n",
    "- GPE (Geopolitical Entities): Substitutes names of countries, cities, states, etc., with random city names using Faker.\n",
    "- DATE: Transforms dates into random dates. Specific dates like 'today', 'tomorrow', or 'next week' are replaced with dates that correspond to these descriptions.\n",
    "- ORG (Organizations): Changes names of organizations, companies, agencies, etc., to random company names generated by Faker.\n",
    "- NORP (Nationalities, Religious or Political Groups): Replaces nationalities, religions, and political group names with random country names, implying a change in nationality.\n",
    "- CARDINAL (Numerals): Alters numerical values to be close to the original number but not exact, within a ±10% range or ±3, whichever is greater.\n",
    "- ORDINAL (Ordinal Numbers): Generates random ordinal numbers (like 1st, 2nd, 3rd, etc.) to replace existing ordinal numbers.\n",
    "- TIME: Changes time mentions to random times. Specific times of the day like 'morning' or 'evening' are replaced with times corresponding to those periods."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8197488583d6da4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"PII_tweet_emotions.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:54:32.997825400Z",
     "start_time": "2024-01-28T12:54:30.887098100Z"
    }
   },
   "id": "521a2db1d5ace81c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:54:39.838125100Z",
     "start_time": "2024-01-28T12:54:32.997825400Z"
    }
   },
   "id": "b05743fe8a77ffb9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from faker import Faker\n",
    "import re\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def close_number(original_number):\n",
    "    try:\n",
    "        num = int(original_number)\n",
    "        # Generate a number within ±10% of the original number or ±3, whichever is greater\n",
    "        percentage_variation = int(num * 0.1)\n",
    "        min_variation = 3  # Minimum variation\n",
    "        variation = max(min_variation, percentage_variation)\n",
    "        return str(random.randint(max(0, num - variation), num + variation))\n",
    "    except ValueError:\n",
    "        # Return the original number if it's not an integer\n",
    "        return original_number\n",
    "    \n",
    "def fake_ordinal():\n",
    "    number = fake.random_int(min=1, max=100)\n",
    "    suffix = [\"th\", \"st\", \"nd\", \"rd\"] + [\"th\"] * 6\n",
    "    return str(number) + suffix[number % 10 if number % 100 not in [11, 12, 13] else 0]\n",
    "    \n",
    "def replace_date(entity_text):\n",
    "    today = datetime.today()\n",
    "    if entity_text.lower() in ['today']:\n",
    "        new_date = fake.date_between(start_date=today, end_date=today)\n",
    "    elif entity_text.lower() in ['tomorrow']:\n",
    "        new_date = fake.date_between(start_date=today + timedelta(days=1), end_date=today + timedelta(days=1))\n",
    "    elif entity_text.lower() in ['next week']:\n",
    "        new_date = fake.date_between(start_date=today + timedelta(days=7), end_date=today + timedelta(days=14))\n",
    "    else:\n",
    "        # For general date entities, return a random future date\n",
    "        new_date = fake.future_date()\n",
    "    \n",
    "    return new_date.strftime(\"%Y-%m-%d\")  # Convert the date to a string\n",
    "\n",
    "\n",
    "def replace_time(entity_text):\n",
    "    # Define time ranges\n",
    "    morning_times = [f\"{hour:02d}:{minute:02d} AM\" for hour in range(6, 12) for minute in range(0, 60)]\n",
    "    evening_times = [f\"{hour:02d}:{minute:02d} PM\" for hour in range(6, 12) for minute in range(0, 60)]\n",
    "\n",
    "    if entity_text.lower() in ['morning']:\n",
    "        return random.choice(morning_times)\n",
    "    elif entity_text.lower() in ['evening', 'tonight']:\n",
    "        return random.choice(evening_times)\n",
    "    else:\n",
    "        # For general time entities, return any random time\n",
    "        return fake.time()\n",
    "\n",
    "def replace_pii_with_fake(text):\n",
    "    # Replace Twitter @username with @ followed by a fake first name\n",
    "    text = re.sub(r'@(\\w+)', lambda x: '@' + fake.first_name(), text)\n",
    "\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Iterate over the identified entities\n",
    "    for ent in doc.ents:\n",
    "        # Replace with fake data based on the entity type\n",
    "        if ent.label_ == 'PERSON':\n",
    "            text = re.sub(re.escape(ent.text), fake.name(), text)\n",
    "        elif ent.label_ == 'GPE':\n",
    "            text = re.sub(re.escape(ent.text), fake.city(), text)\n",
    "        elif ent.label_ == 'DATE':\n",
    "            text = re.sub(re.escape(ent.text), replace_date(ent.text), text)\n",
    "        elif ent.label_ == 'ORG':\n",
    "            text = re.sub(re.escape(ent.text), fake.company(), text)\n",
    "        elif ent.label_ == 'NORP':\n",
    "            text = re.sub(re.escape(ent.text), fake.country(), text)\n",
    "        elif ent.label_ == 'CARDINAL':\n",
    "            text = re.sub(re.escape(ent.text), lambda x: close_number(ent.text), text)\n",
    "        elif ent.label_ == 'ORDINAL':\n",
    "            text = re.sub(re.escape(ent.text), fake_ordinal(), text)\n",
    "        elif ent.label_ == 'TIME':\n",
    "            text = re.sub(re.escape(ent.text), replace_time(ent.text), text)\n",
    "    return text\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['content'] = df['content'].apply(replace_pii_with_fake)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:57:00.485331100Z",
     "start_time": "2024-01-28T12:54:39.837376300Z"
    }
   },
   "id": "3e67063453962912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After being done with anonymizing the eight identified categories, we identify the PII for the anonymized Dataset and take a look at our anonymized dataset by reusing our spaCy from task 3.1.2:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a082ca24a87464fd"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content  \\\n",
      "0  1956967341       empty  @Vincent i know  i was listenin to bad habit e...   \n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696     sadness            Funeral ceremony...gloomy 2024-01-29...   \n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
      "4  1956968416     neutral  @Madeline We want to trade with someone who ha...   \n",
      "5  1956968477       worry  Re-pinging @Julie: why didn't you go to prom? ...   \n",
      "6  1956968487     sadness  I should be sleep, but im not! thinking about ...   \n",
      "7  1956968636       worry               Hmmm. http://www.djhero.com/ is down   \n",
      "\n",
      "                              PII  \n",
      "0                              []  \n",
      "1                              []  \n",
      "2        [('2024-01-29', 'DATE')]  \n",
      "3                              []  \n",
      "4       [('Anthonyburgh', 'ORG')]  \n",
      "5  [('Jonathan Ayala', 'PERSON')]  \n",
      "6             [('5', 'CARDINAL')]  \n",
      "7                              []  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Function to identify PII using spaCy\n",
    "def identify_pii(text):\n",
    "    # Process the text using spaCy to identify named entities\n",
    "    doc = nlp(text)\n",
    "    pii_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return pii_entities\n",
    "\n",
    "pii_original = df['content'].apply(identify_pii)\n",
    "\n",
    "df['PII'] = pii_original\n",
    "df.to_csv(\"Anonymized_PII_tweet_emotions.csv\", index=False)\n",
    "df = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "print(df.iloc[:8])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T12:31:01.639241700Z",
     "start_time": "2024-03-06T12:27:44.506367700Z"
    }
   },
   "id": "3517b896a417891d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the by spaCys model identified PIIs contain our newly anonymized contents. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44e32a3afb85bedc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Analyse the text to determine if any information can be obtained after the transformation process. What conclusions can you draw from this?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d24d03b52d742b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code is designed to first calculate semantic similarity between texts in an original dataset and their anonymized counterparts, assessing how well the anonymization process has retained the original text's meaning. Following this, it aims to check whether any Personally Identifiable Information (PII) from the original dataset remains in the anonymized dataset, ensuring the effectiveness of the anonymization in protecting privacy ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece9a46f291c534d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This first script assesses how well an original dataset's anonymization process preserved semantic content compared to its anonymized version. It employs MobileBert for generating text embeddings, which represent the semantic essence of texts. By calculating the cosine similarity between embeddings of corresponding entries in the original and anonymized datasets, the script quantifies semantic similarity. High similarity scores indicate little to no semantic change, helping evaluate the anonymization's effectiveness. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d34958a9b1ab57"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "from transformers import MobileBertTokenizer, MobileBertModel\n",
    "\n",
    "\n",
    "# Load your datasets\n",
    "df = pd.read_csv(\"tweet_emotions.csv\")  # Make sure you've loaded the original dataset into 'df'\n",
    "df_anonymized = pd.read_csv(\"Anonymized_PII_tweet_emotions.csv\")\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "model = MobileBertModel.from_pretrained('google/mobilebert-uncased').to(device)\n",
    "\n",
    "\n",
    "# Modify the get_embedding function to send inputs to the GPU\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Function to calculate semantic similarity\n",
    "def semantic_similarity(text1, text2, tokenizer, model):\n",
    "    emb1 = get_embedding(text1, tokenizer, model)\n",
    "    emb2 = get_embedding(text2, tokenizer, model)\n",
    "    # Ensure embeddings are 1-D\n",
    "    emb1 = np.squeeze(emb1)\n",
    "    emb2 = np.squeeze(emb2)\n",
    "    #print(text1 + \" and \" + text2)\n",
    "    return 1 - cosine(emb1, emb2)\n",
    "\n",
    "# Calculate similarities\n",
    "try:\n",
    "    similarity_scores = [semantic_similarity(orig, anon, tokenizer, model) for orig, anon in zip(df['content'], df_anonymized['content'])]\n",
    "    df_anonymized['scores'] = similarity_scores\n",
    "except ValueError as e:\n",
    "    print(f\"Error calculating similarity: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T14:42:46.241024600Z",
     "start_time": "2024-03-06T13:47:25.616722200Z"
    }
   },
   "id": "f016ef0d4f4587e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This script evaluates the effectiveness of anonymizing a dataset by comparing the PII fields in the original and anonymized datasets on a row-by-row basis. Using pandas, we load both datasets and check for identical PII entries, marking matches where anonymization may not have been successful. We calculate and report the number of entries that were correctly anonymized versus those that remained unchanged, offering a concise assessment of the anonymization process's success."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ce4ab08eb929de7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 21853 rows with non-empty 'PII' values:\n",
      "- 1033 rows have 'PII' values that match between the original and anonymized datasets.\n",
      "- 20820 rows have 'PII' values that do not match, indicating successful anonymization.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset\n",
    "original_df = pd.read_csv('PII_tweet_emotions.csv')\n",
    "\n",
    "# Load the anonymized dataset\n",
    "anonymized_df = pd.read_csv('Anonymized_PII_tweet_emotions.csv')\n",
    "\n",
    "# Compare 'PII' columns, excluding empty 'PII' lists\n",
    "original_df['Refined_PII_Match'] = (original_df['PII'] == anonymized_df['PII']) & \\\n",
    "                                   (original_df['PII'] != '[]') & \\\n",
    "                                   (anonymized_df['PII'] != '[]')\n",
    "\n",
    "# Calculate matches and non-matches\n",
    "refined_matches = original_df['Refined_PII_Match'].sum()\n",
    "refined_total_non_empty = ((original_df['PII'] != '[]') & (anonymized_df['PII'] != '[]')).sum()\n",
    "refined_non_matches = refined_total_non_empty - refined_matches\n",
    "\n",
    "# Output the results\n",
    "print(f\"Out of {refined_total_non_empty} rows with non-empty 'PII' values:\")\n",
    "print(f\"- {refined_matches} rows have 'PII' values that match between the original and anonymized datasets.\")\n",
    "print(f\"- {refined_non_matches} rows have 'PII' values that do not match, indicating successful anonymization.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T12:44:07.201706900Z",
     "start_time": "2024-03-06T12:44:07.004294900Z"
    }
   },
   "id": "f4a1673cd545cacd",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Conclusion, our anonymisation algorithm effectively anonymized the personally identifiable information (PII) in a tweet dataset, with over 95% of the PII successfully altered. However, around 5% of the original PII remained unchanged, indicating areas where the anonymization process could be improved. There's no direct assessment of whether the anonymized text maintains the original sentiment or meaning, but high semantic similarity scores would suggest the content's contextual integrity is largely preserved.\n",
    "\n",
    "\n",
    "Please continue reading in 3.2.ipynb :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "353d86cd3be615ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
