{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Anonymizing your dataset - 20 marks\n",
    "- Goals: The goal of k-anonymity is to modify a dataset such that any given record cannot be distinguished from at least kâˆ’1 other records regarding certain \"quasi-identifier\" attributes.\n",
    "- Our identified quasi-identifiers from 3.1: 'region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b781a2e32b01fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting point: get a k-anonymized data set as a base\n",
    "- To k-anonymize our given data set, we are using the Mondrian Multidimensional K-Anonymity approach to build on later for t-closeness and l-diversity.\n",
    "- similar to other k-anonymity approaches, a simple and efficient greedy approximation algorithm is implemented to reduce complexity.\n",
    "\n",
    "Because of runtime issues (the athletes.csv runs for more than a few hours with this jupyter notebook) we decided to take the first 20k entries of the athletes dataset to showcase the algorithm. \n",
    "We start by implementing a few functions which will then later be used to k=3 anonymize our dataset: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dfbbc60eed6e1e1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423006\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"athletes.csv\", index_col=False, low_memory=False)\n",
    "print(df.shape[0])\n",
    "\n",
    "# Select the first 10,000 rows\n",
    "df_reduced = df.iloc[:20000, :]\n",
    "\n",
    "# Save the reduced dataset to a new CSV file\n",
    "df_reduced.to_csv(\"reduced_athletes.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.842635200Z",
     "start_time": "2024-01-14T20:43:31.815001300Z"
    }
   },
   "id": "60a056d9854164ad",
   "execution_count": 256
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Some adjustments on the dataset\n",
    "df = pd.read_csv(\"reduced_athletes.csv\", index_col=False, low_memory=False)\n",
    "\n",
    "df['age'].fillna(0, inplace=True)\n",
    "df['height'].fillna(0, inplace=True)\n",
    "df['weight'].fillna(0, inplace=True)\n",
    "\n",
    "df['region'].fillna('', inplace=True)\n",
    "df['gender'].fillna('', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.918313900Z",
     "start_time": "2024-01-14T20:43:33.837027100Z"
    }
   },
   "id": "6f02b8c8ecab030e",
   "execution_count": 257
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "#Calculates and returns the spans (range of values) for each column in a specified partition of a dataframe, with an option to scale these spans by provided values.\n",
    "def calculate_spans(data_frame, data_partition, scaling_factors=None):\n",
    "    column_spans = {}\n",
    "    for column in quasi_identifiers:\n",
    "        if column in categorical:\n",
    "            range_span = len(data_frame[column][data_partition].unique())\n",
    "        else:\n",
    "            range_span = data_frame[column][data_partition].max() - data_frame[column][data_partition].min()\n",
    "        if scaling_factors is not None:\n",
    "            range_span = range_span / scaling_factors[column]\n",
    "        column_spans[column] = range_span\n",
    "    return column_spans"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.928520800Z",
     "start_time": "2024-01-14T20:43:33.919311700Z"
    }
   },
   "id": "6c70412e40d9d7fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Divides a specified partition of a dataframe into two parts based on the median or unique values of a given column, returning a tuple with the indices of these two parts.\n",
    "def split_dataframe_into_two(df, partition_indices, column_name):\n",
    "    partitioned_data = df[column_name][partition_indices]\n",
    "\n",
    "    if column_name in categorical:\n",
    "        unique_values = partitioned_data.unique()\n",
    "        left_values = set(unique_values[:len(unique_values) // 2])\n",
    "        right_values = set(unique_values[len(unique_values) // 2:])\n",
    "        left_partition_indices = partitioned_data.index[partitioned_data.isin(left_values)]\n",
    "        right_partition_indices = partitioned_data.index[partitioned_data.isin(right_values)]\n",
    "        return left_partition_indices, right_partition_indices\n",
    "    else:\n",
    "        median_value = partitioned_data.median()\n",
    "        left_partition_indices = partitioned_data.index[partitioned_data < median_value]\n",
    "        right_partition_indices = partitioned_data.index[partitioned_data >= median_value]\n",
    "        return left_partition_indices, right_partition_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.946726Z",
     "start_time": "2024-01-14T20:43:33.924996100Z"
    }
   },
   "id": "91f3144e071e27cb",
   "execution_count": 259
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Checks if a partition is k-anonymous by comparing its amount of entries with the required (k).\n",
    "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
    "    if len(partition) < k:\n",
    "        return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.956258100Z",
     "start_time": "2024-01-14T20:43:33.938058700Z"
    }
   },
   "id": "b9a2b33d03f26ba2",
   "execution_count": 260
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Partitions a dataframe into valid subsets based on specified feature columns, a sensitive column, and span scales, using a validity function to ensure each partition meets certain criteria.\n",
    "def partition_dataset(df, feature_columns, sensitive_column, scale_factor, validate_partition):\n",
    "    completed_partitions = []\n",
    "    pending_partitions = [df.index]\n",
    "    \n",
    "    while pending_partitions:\n",
    "        current_partition = pending_partitions.pop(0)\n",
    "        column_spans = calculate_spans(df[feature_columns], current_partition, scale_factor)\n",
    "        \n",
    "        for column_name, span_value in sorted(column_spans.items(), key=lambda x: -x[1]):\n",
    "            left_partition_indices, right_partition_indices = split_dataframe_into_two(df, current_partition, column_name)\n",
    "            \n",
    "            if not validate_partition(df, left_partition_indices, sensitive_column) or not validate_partition(df, right_partition_indices, sensitive_column):\n",
    "                continue\n",
    "            \n",
    "            pending_partitions.extend((left_partition_indices, right_partition_indices))\n",
    "            break\n",
    "        else:\n",
    "            completed_partitions.append(current_partition)\n",
    "    \n",
    "    return completed_partitions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.974089800Z",
     "start_time": "2024-01-14T20:43:33.950396Z"
    }
   },
   "id": "45ea8668dea21938",
   "execution_count": 261
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def aggregate_categorical_values(series_categorical):\n",
    "    # Check if the categorical series is empty or if its mode calculation results in an empty series\n",
    "    if series_categorical.empty or series_categorical.mode().empty:\n",
    "        return None  # Return None or a specified default value as a placeholder\n",
    "    else:\n",
    "        # Return the most frequent value in the series (the first element in mode)\n",
    "        return series_categorical.mode().iloc[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.975089500Z",
     "start_time": "2024-01-14T20:43:33.960619Z"
    }
   },
   "id": "d260d0e832101ffd",
   "execution_count": 262
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def aggregate_numerical_series(numerical_series):\n",
    "    # Calculate and return the mean (average) of the numerical series\n",
    "    return numerical_series.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:33.996053600Z",
     "start_time": "2024-01-14T20:43:33.972088800Z"
    }
   },
   "id": "1f48f0e16c8b2940",
   "execution_count": 263
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_anonymized_dataset(dataframe, partition_indices, feature_columns, sensitive_columns, max_partitions=None):\n",
    "    column_aggregations = {}\n",
    "\n",
    "    for column in feature_columns:\n",
    "        if column in categorical:\n",
    "            column_aggregations[column] = aggregate_categorical_values\n",
    "        else:\n",
    "            column_aggregations[column] = aggregate_numerical_series\n",
    "\n",
    "    anonymized_rows = []\n",
    "\n",
    "    # Process each partition\n",
    "    for partition_index, partition in enumerate(partition_indices):\n",
    "        # Limit the number of partitions processed if max_partitions is set\n",
    "        if max_partitions is not None and partition_index > max_partitions:\n",
    "            break\n",
    "\n",
    "        # Aggregate feature columns for the current partition\n",
    "        aggregated_features = dataframe.loc[partition].agg(column_aggregations, squeeze=False)\n",
    "        feature_values = aggregated_features.to_dict()\n",
    "\n",
    "        # Aggregate sensitive columns separately and combine with feature columns\n",
    "        for sensitive_column in sensitive_columns:\n",
    "            sensitive_value_counts = dataframe.loc[partition].groupby(sensitive_column).agg({sensitive_column: 'count'})\n",
    "            \n",
    "            for sensitive_value, count in sensitive_value_counts[sensitive_column].items():\n",
    "                if count == 0:\n",
    "                    continue\n",
    "                \n",
    "                combined_values = feature_values.copy()\n",
    "                combined_values.update({\n",
    "                    sensitive_column: sensitive_value,\n",
    "                    'count': count,\n",
    "                })\n",
    "                anonymized_rows.append(combined_values)\n",
    "\n",
    "    return pd.DataFrame(anonymized_rows)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:34.005916600Z",
     "start_time": "2024-01-14T20:43:33.985930Z"
    }
   },
   "id": "c4cf30a6fb09d064",
   "execution_count": 264
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Starting to k-anonymize with k=3\n",
    "1. First, we define our quasi-identifiers that we chose in 3.1. \n",
    "2. Secondly, we divide our quasi identifiers in two categories\n",
    "   - categorical: schedule, howlong, region, gender, eat\n",
    "      - These attributes need to be treated differently because they cant be compared like numerical attributes\n",
    "   - numerical: age, height, weight\n",
    "      - These attributes need no special treatment\n",
    "3. We then start with k-anonymizing our dataset by calculating the spans of our dataframe and passing the result on to partition our dataset\n",
    "4. the partitions then get aggregated with respect to our categorical and numerical attributes \n",
    "5. the finished dataframe then gets saved to a new .csv "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee717028bfc8d1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#categorical attributes that need to be treated in another way than numerical\n",
    "categorical = {'schedule', 'howlong', 'region', 'gender', 'eat'}\n",
    "for name in categorical:\n",
    "    df[name] = df[name].astype('category')\n",
    "    \n",
    "#Our quasi identifiers that we identified earlier\n",
    "quasi_identifiers = ['region', 'gender', 'age', 'height', 'weight', 'eat', 'schedule', 'howlong']\n",
    "\n",
    "# sensitive-values that should be taken into account\n",
    "sensitive_columns = ['athlete_id', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'run400', 'run5k', 'candj', 'snatch', 'deadlift',\n",
    "                     'backsq', 'pullups']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:43:34.029190300Z",
     "start_time": "2024-01-14T20:43:33.997062800Z"
    }
   },
   "id": "e92b677b7074e723",
   "execution_count": 265
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2910\n"
     ]
    }
   ],
   "source": [
    "full_spans = calculate_spans(df, df.index)\n",
    "finished_partitions = partition_dataset(df, quasi_identifiers, sensitive_columns, full_spans, is_k_anonymous)\n",
    "print(len(finished_partitions))\n",
    "\n",
    "dfn = create_anonymized_dataset(df, finished_partitions, quasi_identifiers, sensitive_columns)\n",
    "\n",
    "dfn.to_csv(\"k_anon.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:44:36.286107900Z",
     "start_time": "2024-01-14T20:43:34.014627900Z"
    }
   },
   "id": "9b0bd1ba37772d37",
   "execution_count": 266
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3adee71c0da5d1d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extending the k-anonymized dataset to also be l-diverse\n",
    "- l-diversity requires that for every group of records sharing  quasi-identifier attributes, there are at least 'l' \"well-represented\" values for the sensitive attribute.\n",
    "- l=2 diversity: Specifically, this means that in each group of records with the same quasi-identifiers, there should be at least two distinct values for the sensitive attributes. The idea is to prevent attackers from deducing the value of a sensitive attribute within a group. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfd5955484476ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_partition_diversity(dataframe, partition_indices, column):\n",
    "    # Calculate the number of unique values in the specified column of the given partition\n",
    "    return len(dataframe.loc[partition_indices, column].unique())\n",
    "\n",
    "def is_l_diverse(dataframe, partition_indices, sensitive_columns_list, l_diversity_threshold=2):\n",
    "    # Check if each sensitive column in the partition meets the l-diversity criterion\n",
    "    for sensitive_column in sensitive_columns_list:\n",
    "        if calculate_partition_diversity(dataframe, partition_indices, sensitive_column) < l_diversity_threshold:\n",
    "            return False\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:44:36.292947700Z",
     "start_time": "2024-01-14T20:44:36.289332400Z"
    }
   },
   "id": "8214935225e196ef",
   "execution_count": 267
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n"
     ]
    }
   ],
   "source": [
    "df_ldiverse = pd.read_csv(\"k_anon.csv\", index_col=False, low_memory=False)\n",
    "full_spans = calculate_spans(df_ldiverse, df_ldiverse.index)\n",
    "\n",
    "finished_l_diverse_partitions = partition_dataset(\n",
    "    df_ldiverse, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    is_l_diverse)\n",
    "print(len(finished_l_diverse_partitions))   #How many partitions are there\n",
    "\n",
    "dfldiverse_finished = create_anonymized_dataset(df_ldiverse, finished_l_diverse_partitions, quasi_identifiers, sensitive_columns)\n",
    "dfldiverse_finished.to_csv(\"l_diverse.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:45:20.194615800Z",
     "start_time": "2024-01-14T20:44:36.293944700Z"
    }
   },
   "id": "32da8f97ac74169c",
   "execution_count": 268
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extending the k-anonymized dataset to also achieve t-closeness\n",
    "- A dataset is said to have t-closeness if the distribution of a sensitive attribute in any given group is not more than 't' different from the distribution of the attribute in the overall dataset.\n",
    "- Unlike l-diversity, which focuses on the variety of sensitive attributes, t-closeness concerns itself with the distribution (frequency) of these attributes.\n",
    "- A smaller t value indicates a stricter requirement for maintaining the distribution of the dataset. (Here: t=0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32954f7c4f93f649"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e927c25a3c6c2d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# calculate t-closeness for numerical values \n",
    "def t_closeness_numerical(df, partition_indices, numerical_column):\n",
    "    full_dataset_values = df[numerical_column]\n",
    "    partition_values = df.loc[partition_indices, numerical_column]\n",
    "\n",
    "    # Check if either the full dataset or the partition is empty\n",
    "    if full_dataset_values.empty or partition_values.empty:\n",
    "        # Return a random float between 30 and 200 if either dataset is empty\n",
    "        return random.uniform(30, 200)\n",
    "\n",
    "    # Compute the Kolmogorov-Smirnov statistic for the two datasets\n",
    "    ks_statistic, _ = ks_2samp(full_dataset_values, partition_values)\n",
    "    return ks_statistic\n",
    "\n",
    "# calculate t-closeness for categorical values\n",
    "def t_closeness_categorical(df, partition_indices, categorical_column, global_frequencies):\n",
    "    total_partition_count = float(len(partition_indices))\n",
    "    max_deviation = None\n",
    "\n",
    "    # Calculate the count of each value in the partition for the specified categorical column\n",
    "    partition_value_counts = df.loc[partition_indices].groupby(categorical_column, observed=False)[categorical_column].agg('count')\n",
    "\n",
    "    # Iterate through each value and calculate its deviation from the global frequencies\n",
    "    for value, count in partition_value_counts.to_dict().items():\n",
    "        partition_frequency = count / total_partition_count\n",
    "        deviation = abs(partition_frequency - global_frequencies[value])\n",
    "\n",
    "        # Update max_deviation if this deviation is greater than the current max\n",
    "        if max_deviation is None or deviation > max_deviation:\n",
    "            max_deviation = deviation\n",
    "\n",
    "    return max_deviation\n",
    "\n",
    "# check if partition is t-close\n",
    "def is_t_close(df, partition_indices, sensitive_columns_list, global_frequencies, threshold=0.2):\n",
    "    # Loop through each sensitive column to calculate the t-closeness\n",
    "    for sensitive_column in sensitive_columns_list:\n",
    "        if sensitive_column not in categorical:\n",
    "            # Calculate t-closeness for numerical columns\n",
    "            closeness_distance = t_closeness_numerical(df, partition_indices, sensitive_column)\n",
    "        else:\n",
    "            # Calculate t-closeness for categorical columns\n",
    "            closeness_distance = t_closeness_categorical(df, partition_indices, sensitive_column, global_frequencies[sensitive_column])\n",
    "        \n",
    "        # Check if the closeness distance exceeds the threshold\n",
    "        if closeness_distance > threshold:\n",
    "            return False\n",
    "\n",
    "    return True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:45:20.201283100Z",
     "start_time": "2024-01-14T20:45:20.198342400Z"
    }
   },
   "id": "c03f0e8542d8b261",
   "execution_count": 269
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950\n"
     ]
    }
   ],
   "source": [
    "df_tclose = pd.read_csv(\"k_anon.csv\", index_col=False, low_memory=False)\n",
    "full_spans = calculate_spans(df_tclose, df_tclose.index)\n",
    "\n",
    "# Get the global frequencies for the sensitive column\n",
    "global_frequencies = {sensitive_column: {} for sensitive_column in sensitive_columns}\n",
    "total_count = len(df)\n",
    "\n",
    "# Determine frequency for every sensitive attribute\n",
    "for sensitive_column in sensitive_columns:\n",
    "    group_counts = df_tclose.groupby(sensitive_column, observed=False)[sensitive_column].agg('count')\n",
    "    for value, count in group_counts.to_dict().items():\n",
    "        p = count / total_count\n",
    "        global_frequencies[sensitive_column][value] = p\n",
    "\n",
    "finished_t_closepartitions = partition_dataset(\n",
    "    df_tclose, quasi_identifiers, sensitive_columns, full_spans,\n",
    "    lambda *args: is_t_close(*args, global_frequencies))\n",
    "print(len(finished_t_closepartitions))\n",
    "\n",
    "\n",
    "dfclose_finished = create_anonymized_dataset(df_tclose, finished_t_closepartitions, quasi_identifiers, sensitive_columns)\n",
    "dfclose_finished.to_csv(\"t_close.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T20:48:29.913649100Z",
     "start_time": "2024-01-14T20:45:20.203246400Z"
    }
   },
   "id": "2790869de5d00a83",
   "execution_count": 270
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discussing the results\n",
    "Our datasets are characterized by three privacy-preserving measures: \n",
    "k=3 anonymity, l=2 diversity, and t=0.2 closeness. These measures are used to ensure that the data can be used for analysis without compromising the privacy of the individuals represented in the data. \n",
    "- k = 3 ensures that any individual's data cannot be distinguished from at least two other individuals within the dataset. Protects against identification risks.\n",
    "- l = 2 diversity: ensures variety in the sensitive attribute in each group of the dataset. In a dataset with l=2 diversity, each group of records must have at least two different values for the sensitive attribute. Protects against attribute disclosure.\n",
    "- t = 0.2  ensures the preservation of data utility by maintaining a consistent distribution of sensitive attributes.\n",
    "By combining these three techniques, we can achieve decent privacy protection of our given dataset. Each measure addresses a different aspect of privacy.\n",
    "\n",
    "The rest of the comparison / discussion of results happen in 3.4. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4fb78b577dcd6ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
